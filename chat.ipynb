{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'french'\n",
    "setting = \"Normandie viking du 9e siècle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frieder/Documents/GitHub/procedural-stories/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 93206.76it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 85101.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
    "from langchain_community.chat_models.mlx import ChatMLX\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.globals import set_verbose\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "#set_debug(True)\n",
    "#set_verbose(True)\n",
    "\n",
    "print('Loading model...')\n",
    "# Load model from huggingface, using the MLX framework to take advantage of Apple Silicon\n",
    "llm = MLXPipeline.from_model_id(\n",
    "    \"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\",\n",
    "    #\"mlx-community/Meta-Llama-3.1-8B-Instruct-bf16\",\n",
    "    #\"mlx-community/Qwen2.5-32B-Instruct-4bit\", # Let's try using a larger model, see if that improves results\n",
    "    #\"mlx-community/Llama-3.2-3B-Instruct-8bit\",\n",
    "    pipeline_kwargs={\"max_tokens\": 2048, \"temp\": 0.2, \"repetition_penalty\":1.2},\n",
    ")\n",
    "\n",
    "predictable_llm = MLXPipeline.from_model_id(\n",
    "    \"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\",\n",
    "    pipeline_kwargs={\"max_tokens\": 2048, \"temp\": 0, \"repetition_penalty\":1.2},\n",
    ")\n",
    "\n",
    "# Setup verbose mode: https://stackoverflow.com/a/77629872/10914628\n",
    "model = ChatMLX(llm=llm, verbose=True).with_config({'callbacks': [ConsoleCallbackHandler()]})\n",
    "\n",
    "predictable_model = ChatMLX(llm=predictable_llm)\n",
    "print('Model loaded.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Running HermiT...\n",
      "    java -Xmx2000M -cp /Users/frieder/Documents/GitHub/procedural-stories/.venv/lib/python3.12/site-packages/owlready2/hermit:/Users/frieder/Documents/GitHub/procedural-stories/.venv/lib/python3.12/site-packages/owlready2/hermit/HermiT.jar org.semanticweb.HermiT.cli.CommandLine -c -O -D -I file:////var/folders/92/x0b2qb4n7b93p8w6v3hxvtc80000gp/T/tmpuq6hllw_\n",
      "* Owlready2 * HermiT took 6.727673768997192 seconds\n",
      "* Owlready * Reparenting story_poptest.isEnemyWith: {owl.IrreflexiveProperty, owl.SymmetricProperty, story_poptest.hasRelationship, owl.ObjectProperty} => {owl.IrreflexiveProperty, story_poptest.hasRelationship, owl.SymmetricProperty}\n",
      "* Owlready * Reparenting story_poptest.hasFriendshipWith: {owl.TransitiveProperty, owl.SymmetricProperty, story_poptest.hasRelationship, owl.ObjectProperty} => {owl.TransitiveProperty, story_poptest.hasRelationship, owl.SymmetricProperty}\n",
      "* Owlready * Reparenting story_poptest.hasAllegiance: {story_poptest.hasRelationship, owl.FunctionalProperty, owl.ObjectProperty} => {story_poptest.hasRelationship, owl.FunctionalProperty}\n",
      "* Owlready * Reparenting story_poptest.knows: {story_poptest.hasRelationship, owl.ObjectProperty} => {story_poptest.hasRelationship}\n",
      "* Owlready * Reparenting story_poptest.isRulerOf: {owl.InverseFunctionalProperty, story_poptest.hasRelationship, owl.ObjectProperty} => {owl.InverseFunctionalProperty, story_poptest.hasRelationship}\n",
      "* Owlready * Reparenting story_poptest.loves: {story_poptest.hasRelationship, owl.ObjectProperty} => {story_poptest.hasRelationship}\n",
      "* Owlready * Reparenting story_poptest.containsCharacter: {owl.InverseFunctionalProperty, story_poptest.containsEntity, owl.ObjectProperty} => {story_poptest.containsEntity}\n",
      "* Owlready * Reparenting story_poptest.containsItem: {owl.InverseFunctionalProperty, story_poptest.containsEntity, owl.ObjectProperty} => {story_poptest.containsEntity}\n",
      "* Owlready * Reparenting story_poptest.characterIsLocatedAt: {story_poptest.isLocatedAt, owl.FunctionalProperty, owl.ObjectProperty} => {story_poptest.isLocatedAt}\n",
      "* Owlready * (NB: only changes on entities loaded in Python are shown, other changes are done but not listed)\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import *\n",
    "\n",
    "onto = get_ontology('file://story_poptest.rdf').load()\n",
    "\n",
    "# Verify that the story world is internally coherent, at least by OWL standards\n",
    "with onto:\n",
    "    sync_reasoner() \n",
    "\n",
    "    # for l in onto.CurrentLocation.instances(): print(f\"CurrentLocation: {l.hasName}\")\n",
    "    # for c in onto.Character.instances(): print(c.hasName)\n",
    "\n",
    "    # print(onto.Player.instances()[0].INDIRECT_isLocatedAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote, unquote\n",
    "\n",
    "def encode_entity_name(name: str) -> str:\n",
    "    return quote(name.lower())\n",
    "\n",
    "def decode_entity_name(encoded_name: str) -> str:\n",
    "    return unquote(encoded_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "def find_levenshtein_match(string: str, entity_list: list, threshold: int = 5):\n",
    "    \"\"\"Find a match amongst a list of entities from the ontology, in case the LLM made slight typos !\"\"\"\n",
    "    minimum = threshold\n",
    "    closest_entity = None\n",
    "    for entity in entity_list:\n",
    "        dist = distance(string, entity.hasName)\n",
    "        if dist < minimum:\n",
    "            minimum = dist\n",
    "            closest_entity = entity\n",
    "    return closest_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu te trouves au coeur de la Forêt Abandonnée, un lieu où la nature a repris son droit après avoir été dévastée par l'épidémie zombie. Les arbres gémissent sous le vent, leurs branches cassantes semblent menaçantes, tandis que les feuilles mortes crissent sous tes pieds. La lumière filtrant entre les feuilles forme des motifs étranges sur le sol, créant une atmosphère mystérieuse.\n",
      "\n",
      "Tu es Alexandre Durand, un ancien biologiste spécialisé dans les maladies virales. Tu as été choisi pour explorer les zones contaminées alentour du Village des Survivants, là où ta famille t'attend. Le but de ton voyage est simple : trouver une source sécurisée de nourriture ou d'eau potable pour assurer la survie immédiate du Village. Sans ces éléments vitaux, tout ce que tu feras sera vain.\n",
      "\n",
      "La Forêt Abandonnée semble être un bon début. On dit qu'elle abrite encore quelques sources d'eau pure, ainsi que des plantes médicinales capables de guérir les blessures. Mais attention, tu n'es pas seul ici. D'autres personnes ont également trouvé refuge dans cet endroit hostile, chacun poursuivant leurs objectifs secrets... \n",
      "\n",
      "Que fais-tu maintenant?\n"
     ]
    }
   ],
   "source": [
    "print(adventure_start.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ! Now we have the first message that will be sent when the player starts their game !\n",
    "Let's get onto the conversation loop now !\n",
    "\n",
    "# Conversation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_SYSTEM_PROMPT = \"\"\"\n",
    "You are an LLM designed to act as the engine for a text adventure game set in \"{{setting}}\".\n",
    "\n",
    "Keep in mind that more things can and should be revealed later, after interaction with the player, so feel free to keep some information to yourself for later.\n",
    "Keep in mind that not everything that is in a location is necessarily immediately visible or known to the player. Things can be hidden within a location, and a location can be quite large.\n",
    "\n",
    "The game is played by interactions between the game (you) and the player. The player will type in natural language whatever they want to do, etc.\n",
    "Do not reveal everything all at once: let the player discover things. Only output natural text, as one would read in a book they actually were the hero of.\n",
    "\n",
    "Your messages should be short. Please do not produce lengthy messages. Your messages should be one to two sentences long. The player can always ask for more details ! For dialogues, you will output only one line of dialogue, and let the player respond.\n",
    "\n",
    "Use the game elements provided.\n",
    "\n",
    "Always answer in {{language}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_MESSAGE_TEMPLATE = \"\"\"\n",
    "[INST]\n",
    "The player's current location is \"{{location.hasName}}\". {{location.hasName}} is described as \"{{location.hasDescription}}\".\n",
    "\n",
    "The locations accessible from where the player is are {{nearby_locations_names}}. You should discreetly tell the player they can go there, without being too explicit.\n",
    "\n",
    "The player's character is named \"{{player.hasName}}\" and is described as \"{{player.hasDescription}}\". The player's goal is \"{{player.hasGoal.hasDescription}}\"\n",
    "\n",
    "Characters present in {{location.hasName}}:\n",
    "{%- for character in characters_nearby %}\n",
    "    - {{ character.hasName }}: {{character.hasDescription}} (narrative importance {{character.hasImportance}})\n",
    "{%- endfor %}\n",
    "\n",
    "Items present in {{location.hasName}}:\n",
    "{%- for item in items_nearby %}\n",
    "    - {{ item.hasName }}: {{item.hasDescription}} (narrative importance {{item.hasImportance}})\n",
    "{%- endfor %}\n",
    "\n",
    "Answer directly and briefly to the user's message. You answer should be one or two sentences at most ! If the player is curious they will ask.\n",
    "[/INST]\n",
    "The player's message:\n",
    "{{message}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: The reasoner will use the state of the ontology at the moment it is instantiated.\n",
    "If the player moves to a new location, the reasoner will not be updated with this new information.\n",
    "\n",
    "Source: https://stackoverflow.com/a/77629872/10914628 https://dice-group.github.io/owlapy/usage/reasoning_details.html\n",
    "\n",
    "Solution: Use isolated worlds, one that is clean, which we update and change with asserted facts, the second one that is used for inference.\n",
    "When the player moves, we change the clean world, and use it for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html\n",
    "from typing import Optional, List\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    ConfigurableFieldSpec,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"In memory implementation of chat message history.\"\"\"\n",
    "\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        \"\"\"Add a list of messages to the store\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "\n",
    "# Here we use a global variable to store the chat message history.\n",
    "# This will make it easier to inspect it to see the underlying results.\n",
    "store = {}\n",
    "\n",
    "def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return store[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", CHAT_SYSTEM_PROMPT),\n",
    "    ('ai', adventure_start.content),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{{message}}\"),\n",
    "], template_format='jinja2')\n",
    "\n",
    "chain = chat_prompt | model\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_by_session_id,\n",
    "    input_messages_key=\"message\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve relevant information from the KG\n",
    "with onto:\n",
    "    player = list(onto.Player.instances())[0]\n",
    "    current_location = player.INDIRECT_isAtLocation\n",
    "    characters_nearby = current_location.INDIRECT_containsCharacter\n",
    "    locations_nearby = current_location.INDIRECT_isLinkedToLocation\n",
    "    nearby_locations_names = [l.hasName for l in locations_nearby]\n",
    "    items_nearby = current_location.INDIRECT_containsItem\n",
    "\n",
    "\n",
    "    print(chain_with_history.invoke(  # noqa: T201\n",
    "        {\"setting\": setting, \n",
    "        \"language\": language, \n",
    "        'characters_nearby': characters_nearby,\n",
    "        'items_nearby': items_nearby,\n",
    "        'player': player,\n",
    "        'nearby_locations_names': nearby_locations_names,\n",
    "        \"message\": \"Aller à la forge\"},\n",
    "        config={\"configurable\": {\"session_id\": \"foo\"}}\n",
    "    ).content)\n",
    "\n",
    "# Uses the store defined in the example above.\n",
    "print(store)  # noqa: T201\n",
    "\n",
    "# print(chain_with_history.invoke(  # noqa: T201\n",
    "#     {\"setting\": setting, \"question\": \"What's its inverse\"},\n",
    "#     config={\"configurable\": {\"session_id\": \"foo\"}}\n",
    "# ))\n",
    "\n",
    "# print(store)  # noqa: T201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 63e1d314-8406-4a33-bece-60875db6a5c7 not found for run cbe6685b-7c43-48e4-b032-22b3f603f795. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatMLX] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: \\nYou are an LLM designed to act as the engine for a text adventure game set in \\\"Normandie viking du 9e siècle\\\".\\n\\nKeep in mind that more things can and should be revealed later, after interaction with the player, so feel free to keep some information to yourself for later.\\nKeep in mind that not everything that is in a location is necessarily immediately visible or known to the player. Things can be hidden within a location, and a location can be quite large.\\n\\nThe game is played by interactions between the game (you) and the player. The player will type in natural language whatever they want to do, etc.\\nDo not reveal everything all at once: let the player discover things. Only output natural text, as one would read in a book they actually were the hero of.\\n\\nYour messages should be short. Please do not produce lengthy messages. Your messages should be one to two sentences long. The player can always ask for more details ! For dialogues, you will output only one line of dialogue, and let the player respond.\\n\\nUse the game elements provided.\\n\\nAlways answer in french\\nAI: **Bienvenue à Gravepine**\\n\\nTu te trouves debout devant le village de Gravepine, sous le soleil levant qui éclaire les toits de chaume et les murs de bois épais. Le village est paisible, avec quelques habitants qui s'éveillent lentement, tandis que les chiens bercent leurs maîtres. Tu es Erik Stormbringer, un jeune Viking ambitieux originaire de ce petit coin de Normandie. Ton objectif est de gagner la reconnaissance nécessite pour diriger ta propre flotte et conquérir un nouveau territoire.\\n\\nThora la Forgeonne, la plus grande forgeresse du village, te regarde depuis la fenêtre de sa forge, ses yeux brillants de curiosité. Elle te fait signe de venir lui parler. Et puis, tu vois le grand hall des Jarls, majestueusement décoré de trophées de bataille et de runes magiques, attirer l'attention de tous les habitants du village.\\n\\nLe vent apporte l'odeur des poissons frais du port de Gravepine, où les navigateurs sont déjà en train de préparer leurs embarcations pour les prochaines croisières. Mais toi, tu n'es pas là pour simplement profiter de ces activités quotidiennes. Tu as un plan, et il compte beaucoup.\\n\\nQu'est-ce que tu veux faire?\\n\\n(Remarque : tu peux demander à voir les options disponibles en tapant `options` ou `h`)\\nHuman: Aller à la forge\\nAI: Tu marches vers la forge de Thora, son rythme d'étincelles et de coups de hache résonnant dans ton dos. La porte de la forge s'ouvre sur une pièce fumée et bruyante, où Thora t'accueille avec un sourire chaleureux. \\n\\n\\\"Ah, Erik Stormbringer! Je vois que vous êtes prêt à commencer votre quête. Vous avez besoin d'un nouvel équipement? Ou peut-être des informations sur vos parents disparus?\\\"\\n\\nElle te tend la main, invitant à entrer dans la forge. Un fourneau arde au centre de la pièce, et plusieurs outils et métaux scintillent sur les étagères.\\nHuman: Thora, je compte partir conquerrir des terres. Va tu m'apporter ton soutien ?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatMLX] [6.85s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Thora te regarde avec intensité, comme si elle pesait tes paroles. Son visage se voit plissé en un sourire dédaigneux, mais aussi en un peu de respect.\\n\\n\\\"Erik Stormbringer, vous voulez conquérir des terres? C'est un chemin difficile, plein de dangers et de défis. Mais... je suis fière de vous, même si vous ne ressemblez pas exactement aux autres Vikings de notre village. Votre père était un guerrier redoutable, et j'ai confiance en vous.\\\"\\n\\nElle frappe le poing contre le rebord de la forge, faisant trembler les outils. \\\"Je vais vous aider, oui. Mais nous devons être prudent. Il y a ceux qui ne souhaiteraient pas que vous réussissez.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Thora te regarde avec intensité, comme si elle pesait tes paroles. Son visage se voit plissé en un sourire dédaigneux, mais aussi en un peu de respect.\\n\\n\\\"Erik Stormbringer, vous voulez conquérir des terres? C'est un chemin difficile, plein de dangers et de défis. Mais... je suis fière de vous, même si vous ne ressemblez pas exactement aux autres Vikings de notre village. Votre père était un guerrier redoutable, et j'ai confiance en vous.\\\"\\n\\nElle frappe le poing contre le rebord de la forge, faisant trembler les outils. \\\"Je vais vous aider, oui. Mais nous devons être prudent. Il y a ceux qui ne souhaiteraient pas que vous réussissez.\\\"\",\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-cbe6685b-7c43-48e4-b032-22b3f603f795-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "Thora te regarde avec intensité, comme si elle pesait tes paroles. Son visage se voit plissé en un sourire dédaigneux, mais aussi en un peu de respect.\n",
      "\n",
      "\"Erik Stormbringer, vous voulez conquérir des terres? C'est un chemin difficile, plein de dangers et de défis. Mais... je suis fière de vous, même si vous ne ressemblez pas exactement aux autres Vikings de notre village. Votre père était un guerrier redoutable, et j'ai confiance en vous.\"\n",
      "\n",
      "Elle frappe le poing contre le rebord de la forge, faisant trembler les outils. \"Je vais vous aider, oui. Mais nous devons être prudent. Il y a ceux qui ne souhaiteraient pas que vous réussissez.\"\n"
     ]
    }
   ],
   "source": [
    "player_message = \"Thora, je compte partir conquerrir des terres. Va tu m'apporter ton soutien ?\"\n",
    "\n",
    "with onto:\n",
    "    player = list(onto.Player.instances())[0]\n",
    "    current_location = player.INDIRECT_isAtLocation\n",
    "    characters_nearby = current_location.INDIRECT_containsCharacter\n",
    "    locations_nearby = current_location.INDIRECT_isLinkedToLocation\n",
    "    nearby_locations_names = [l.hasName for l in locations_nearby]\n",
    "    items_nearby = current_location.INDIRECT_containsItem\n",
    "\n",
    "\n",
    "    print(chain_with_history.invoke(  # noqa: T201\n",
    "        {\"setting\": setting, \n",
    "        \"language\": language, \n",
    "        'characters_nearby': characters_nearby,\n",
    "        'items_nearby': items_nearby,\n",
    "        'player': player,\n",
    "        'nearby_locations_names': nearby_locations_names,\n",
    "        \"message\": player_message},\n",
    "        config={\"configurable\": {\"session_id\": \"foo\"}}\n",
    "    ).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we will pre-generate the first AI message, which lays out the start of the game.\n",
    "\n",
    "Then, all subsequent turns will use the same system prompt, as well as dynamically enhancing the player's message with relevant information from the KG\n",
    "\n",
    "After the player sends their message, we either can:\n",
    "\n",
    "- Have the LLM respond without knowledge, then analyze the message pair for changes to the KG\n",
    "- Have the LLM analyze the user's message for intent (especially moving to new locations, which will require the information of the new location to generate the response)\n",
    "\n",
    "I think the second approach is just better, especially in the case of moving to new locations, which change the knowledge necessary to generate the response.\n",
    "\n",
    "\n",
    "\n",
    "Once the AI response is generated, we also want to extract changes to the world from the message pair, based on the AI response (item lost, item gained)\n",
    "\n",
    "\n",
    "So... Basically we need to do intent analysis of the Human message.\n",
    "\n",
    "\n",
    "Let's summarize a typical conversation turn:\n",
    "\n",
    "0. The player reads the game's message and decides on something to do.\n",
    "1. The player's message is analyzed with a prompt, made to extract actionable intent that requires new knowledge (move to other location)\n",
    "2. If the player moves to another location, we need to retrieve the information about this new location and use that info to generate the AI response. At the same time, we update the KG (player's position)\n",
    "3. Generate the game response\n",
    "4. While the player reads and decides on their next turn and types it, analyze the last (message, response) pair for changes to the world: updates to the player's inventory (remove, add or change item's description), updates to characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move intent\n",
    "Immediately after the player's message, we need to know if we need to load information about a different location, in order to guide the generation of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENT_ANALYSIS_TEMPLATE = \"\"\"\n",
    "Human: Please analyze the player's message and determine if they **explicitly** intend to move to a nearby location **immediately**.\n",
    "Nearby locations: {{nearby_locations_names}}\n",
    "Player message: \"{{player_message}}\"\n",
    "\n",
    "**Please output only the *exact location name* or 'none' with ABSOLUTELY no additional explanation, notes or details. If it doesn't fit, output 'none'**\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human: Please analyze the player's message and determine if they **explicitly** intend to move to a nearby location **immediately**.\n",
      "Nearby locations: \"Mine Délaissée\", \"Lac Pollué\", \"Village des Survivants\"\n",
      "Player message: \"Retourner à la maison\"\n",
      "\n",
      "**Please output only the *exact location name* or 'none' with ABSOLUTELY no additional explanation, notes or details. If it doesn't fit, output 'none'**\n",
      "Output:\n",
      "Output: mine délaissée\n",
      "mine délaissée\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "story_poptest.mine%20d%C3%A9laiss%C3%A9e"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_move_intent(message: str, onto) -> Optional[onto.Location]:\n",
    "    prompt = PromptTemplate(\n",
    "        template=INTENT_ANALYSIS_TEMPLATE,\n",
    "        template_format='jinja2'\n",
    "    )\n",
    "\n",
    "    with onto:\n",
    "        nearby_locations = onto.Player.instances()[0].INDIRECT_isLocatedAt.INDIRECT_isLinkedToLocation\n",
    "        \n",
    "        nearby_locations_names = []\n",
    "        for l in nearby_locations:\n",
    "            if l.name == \"CurrentLocation\":\n",
    "                continue\n",
    "            \n",
    "            nearby_locations_names.append(l.hasName)\n",
    "\n",
    "        #print(nearby_locations_names)\n",
    "        \n",
    "        print(prompt.invoke({\n",
    "            'nearby_locations_names': \", \".join(f'\"{l}\"' for l in nearby_locations_names),\n",
    "            'player_message': message,\n",
    "            'language': language\n",
    "        }).text)\n",
    "        \n",
    "        chain = prompt | predictable_model\n",
    "        \n",
    "        analysis = chain.invoke({\n",
    "            'nearby_locations_names': \", \".join(f'\"{l}\"' for l in nearby_locations_names),\n",
    "            'player_message': message,\n",
    "            'language': language\n",
    "        })\n",
    "        print(f\"Output: {analysis.content}\")\n",
    "        \n",
    "        \n",
    "        result = analysis.content.strip().strip('\"')\n",
    "        \n",
    "        \n",
    "        \n",
    "        if result.lower() == 'none':\n",
    "            return None\n",
    "        \n",
    "        print(result)\n",
    "        \n",
    "        entity = find_levenshtein_match(result, onto.Location.instances())\n",
    "        \n",
    "        return entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human: Please analyze the player's message and determine if they **explicitly** intend to move to a nearby location **immediately**.\n",
      "Nearby locations: \"Mine Délaissée\", \"Lac Pollué\", \"Village des Survivants\"\n",
      "Player message: \"Aller à la mine\"\n",
      "\n",
      "**Please output only the *exact location name* or 'none' with ABSOLUTELY no additional explanation, notes or details. If it doesn't fit, output 'none'**\n",
      "Output:\n",
      "Output: Mine Délaissée\n",
      "Mine Délaissée\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "story_poptest.mine%20d%C3%A9laiss%C3%A9e"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_move_intent('Aller à la mine', onto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the response, we need to report any change to the world state into the knowledge graph.\n",
    "\n",
    "A player needs to be able to interact with the graph in a few ways:\n",
    "- move to a nearby location\n",
    "- claim ownership (of an unowned item): take, buy\n",
    "- change ownership (of an owned item): give, drop, sell\n",
    "- change in emotional state of any character\n",
    "- change in relationship between characters\n",
    "- update item description (for example if an item deteriorates, etc)\n",
    "- update character description (for example if a character needs to remember something)\n",
    "- update location description (for example if the player changed the location in some notable way)\n",
    "- trigger event\n",
    "- update events memory (?)\n",
    "- death of a character (?)\n",
    "\n",
    "But these are a lot of tasks to ask from one prompt to a small model like Llama3.1-8B-8bit\n",
    "\n",
    "We can classify these tasks in groups:\n",
    "- Inventory related tasks\n",
    "- Character related tasks\n",
    "- Location related tasks\n",
    "\n",
    "Now, we can imagine using a prompt for each one of these categories, in order to get a fair balance of speed and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION_ACTIONS_PARSER_TEMPLATE = \"\"\"\n",
    "Human: Please analyze the player's message and determine if they intend to interact with a location.\n",
    "\n",
    "**Action Types**:\n",
    "- \"move\" if the player wants to go to a nearby location immediately.\n",
    "- \"inspect\" if the player wants to examine or observe the current location.\n",
    "- \"modify\" if the player intends to alter or change the location in some way.\n",
    "\n",
    "**Nearby locations**: {{nearby_locations_names}}\n",
    "Player message: \"{{player_message}}\"\n",
    "Game response: \"{{game_response}}\"\n",
    "\n",
    "**Please output one of the following:**\n",
    "- For **move**: output the exact name of the nearby location.\n",
    "- For **inspect**: output \"inspect\".\n",
    "- For **modify**: output \"modify\".\n",
    "- If none of these fit, output \"none\".\n",
    "\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVENTORY_ACTIONS_PARSER_TEMPLATE = \"\"\"\n",
    "Human: Please analyze the player's message and determine if they intend to interact with an item.\n",
    "\n",
    "**Action Types**:\n",
    "- \"claim\" if the player wants to take or pick up an unowned item.\n",
    "- \"give\" if the player intends to give an item they own to another character.\n",
    "- \"drop\" if the player intends to leave an item they own in the current location.\n",
    "- \"destroy\" if an item \n",
    "\n",
    "**Owned items**: {{player_owned_items}}\n",
    "**Unowned items in the location**: {{unowned_items}}\n",
    "\n",
    "Player message: \"{{player_message}}\"\n",
    "Game response: \"{{game_response}}\"\n",
    "\n",
    "**Please output one of the following:**\n",
    "- For **claim**: output \"take: [item_name]\" (e.g., \"take: map\").\n",
    "- For **give**: output \"give: [item_name]\" (e.g., \"give: coin\").\n",
    "- For **drop**: output \"drop: [item_name]\" (e.g., \"drop: torch\").\n",
    "- If none of these fit, output \"none\".\n",
    "\n",
    "Output:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARACTER_ACTIONS_PARSER_TEMPLATE = \"\"\"\n",
    "Human: Please analyze the player's message and corresponding game response to determine if the interaction changed something about the characters present in the scene\n",
    "\n",
    "**Action Types**:.\n",
    "- \"relationship\" if the relationship between two characters has changed\n",
    "\n",
    "**Characters present**: {{characters_present}}\n",
    "Player message: \"{{player_message}}\"\n",
    "Game response: \"{{game_response}}\"\n",
    "\n",
    "**Please output one of the following:**\n",
    "- For **relationship**: output \"relationship: [character_name]\" (e.g., \"relationship: Scrooge Mcduck\").\n",
    "- If none of these fit, output \"none\".\n",
    "\n",
    "Output:\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
